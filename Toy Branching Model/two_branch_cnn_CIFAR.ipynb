{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "from cca_core import flatten_weights\n",
    "from scipy.linalg import subspace_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoBranchCNN_CIFAR(\n",
      "  (conv1): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (branch1_conv1): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (branch1_conv2): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (branch1_conv3): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (branch1_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (branch1_fc1): Linear(in_features=2000, out_features=50, bias=True)\n",
      "  (branch2_conv1): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (branch2_conv2): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (branch2_conv3): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (branch2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (branch2_fc1): Linear(in_features=2000, out_features=50, bias=True)\n",
      "  (final_fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TwoBranchCNN_CIFAR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoBranchCNN_CIFAR, self).__init__()\n",
    "        # Initial common layer\n",
    "        self.conv1 = nn.Conv2d(3, 20, kernel_size=5)  # Adjusted for 3-channel input\n",
    "        \n",
    "        # Branch 1\n",
    "        self.branch1_conv1 = nn.Conv2d(20, 20, kernel_size=5)\n",
    "        self.branch1_conv2 = nn.Conv2d(20, 40, kernel_size=3, padding=1)\n",
    "        self.branch1_conv3 = nn.Conv2d(40, 80, kernel_size=3, padding=1)\n",
    "        self.branch1_drop = nn.Dropout2d()\n",
    "        self.branch1_fc1 = nn.Linear(2000, 50)  # Adjusted for flattened output size\n",
    "\n",
    "        # Branch 2\n",
    "        self.branch2_conv1 = nn.Conv2d(20, 20, kernel_size=5)\n",
    "        self.branch2_conv2 = nn.Conv2d(20, 40, kernel_size=3, padding=1)\n",
    "        self.branch2_conv3 = nn.Conv2d(40, 80, kernel_size=3, padding=1)\n",
    "        self.branch2_drop = nn.Dropout2d()\n",
    "        self.branch2_fc1 = nn.Linear(2000, 50)  # Adjusted for flattened output size\n",
    "        \n",
    "        # Final classifier\n",
    "        self.final_fc = nn.Linear(100, 10)  # Output for CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        \n",
    "        # Branch 1\n",
    "        b1 = F.relu(F.max_pool2d(self.branch1_conv1(x), 2))\n",
    "        b1 = F.relu(self.branch1_conv2(b1))\n",
    "        b1 = F.relu(self.branch1_conv3(b1))\n",
    "        b1 = self.branch1_drop(b1)\n",
    "        b1 = b1.view(-1, self.num_flat_features(b1))\n",
    "        b1 = F.relu(self.branch1_fc1(b1))\n",
    "        \n",
    "        # Branch 2\n",
    "        b2 = F.relu(F.max_pool2d(self.branch2_conv1(x), 2))\n",
    "        b2 = F.relu(self.branch2_conv2(b2))\n",
    "        b2 = F.relu(self.branch2_conv3(b2))\n",
    "        b2 = self.branch2_drop(b2)\n",
    "        b2 = b2.view(-1, self.num_flat_features(b2))\n",
    "        b2 = F.relu(self.branch2_fc1(b2))\n",
    "        \n",
    "        # Combine branches\n",
    "        combined = torch.cat((b1, b2), dim=1)\n",
    "        output = self.final_fc(combined)\n",
    "        return F.log_softmax(output, dim=1)\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "# Create the model instance\n",
    "model = TwoBranchCNN_CIFAR()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize each channel\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_data = datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load testing data\n",
    "test_data = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader setups\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # Clear the gradients of all optimized variables\n",
    "        output = model(data)  # Forward pass\n",
    "        loss = criterion(output, target)  # Calculate loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        total_loss += loss.item()  # Sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "    print(f'\\nTraining set: Average loss: {total_loss / len(train_loader):.4f}, Accuracy: {correct}/{len(train_loader.dataset)} ({100. * correct / len(train_loader.dataset):.0f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # No gradients tracked\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 1 before dropout: torch.Size([1, 80, 5, 5])\n",
      "Branch 1 after flattening: torch.Size([1, 2000])\n",
      "Branch 2 before dropout: torch.Size([1, 80, 5, 5])\n",
      "Branch 2 before dropout: torch.Size([1, 2000])\n"
     ]
    }
   ],
   "source": [
    "# Debugging code to check the output size\n",
    "# add print statements at layers to check the output size e.g. print(\"branch1 before drop\", b1.size())\n",
    "# Assuming the input size for CIFAR-10 (3 channels, 32x32 images)\n",
    "# dummy_input = torch.randn(1, 3, 32, 32)  # Batch size of 1\n",
    "# output = model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.315181\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.906210\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.713354\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.634667\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.614623\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.402346\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.363103\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.522400\n",
      "\n",
      "Training set: Average loss: 1.5937, Accuracy: 20521/50000 (41%)\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.493443\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.336637\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.077846\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.179010\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.409663\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.334902\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.253246\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.121558\n",
      "\n",
      "Training set: Average loss: 1.2360, Accuracy: 27768/50000 (56%)\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.863044\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.032058\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.151633\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.906357\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.769950\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.893421\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.167655\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.941973\n",
      "\n",
      "Training set: Average loss: 1.0821, Accuracy: 30810/50000 (62%)\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.049529\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.864241\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.069552\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.132619\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.945075\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.033851\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.024413\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.912316\n",
      "\n",
      "Training set: Average loss: 0.9840, Accuracy: 32540/50000 (65%)\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.969983\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.837656\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.721883\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.036714\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.872632\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.017319\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.884547\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.806419\n",
      "\n",
      "Training set: Average loss: 0.9123, Accuracy: 33965/50000 (68%)\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.929309\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.748506\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.565955\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.666280\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 1.105392\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.724699\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.759743\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.947446\n",
      "\n",
      "Training set: Average loss: 0.8527, Accuracy: 35016/50000 (70%)\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.892309\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.654472\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.721339\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.933261\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.036582\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.552984\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.774424\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.878981\n",
      "\n",
      "Training set: Average loss: 0.8040, Accuracy: 35750/50000 (72%)\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.769976\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.614689\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.791932\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.865455\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.836700\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.698511\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.790982\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.813874\n",
      "\n",
      "Training set: Average loss: 0.7628, Accuracy: 36534/50000 (73%)\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.674632\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.701464\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.628541\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.437807\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.741682\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.492627\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.790847\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.726380\n",
      "\n",
      "Training set: Average loss: 0.7205, Accuracy: 37205/50000 (74%)\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.858097\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.777927\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.523011\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.725758\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.695361\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.450646\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.659326\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.636889\n",
      "\n",
      "Training set: Average loss: 0.6903, Accuracy: 37669/50000 (75%)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TwoBranchCNN_CIFAR().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 11):  # Run for 10 epochs\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "\n",
    "#torch.save(model.state_dict(), 'model_weights_two_stream_CIFAR10.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.8329, Accuracy: 7188/10000 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, device, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'model_weights_two_stream_CIFAR10.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
